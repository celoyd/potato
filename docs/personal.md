# Personal notes on pansharpening

## How big are pictures?

High-resolution satellite images have a way of annoying people. They don’t fit well in everyday categories. The linguist David Zubin [once proposed](https://escholarship.org/content/qt44q4w68s/qt44q4w68s_noSplash_1c9886bb9a4650047662d293c0f8d8aa.pdf?t=nsvvtu) a typology of scales – as we understand them, not as we measure them – that I feel gets at something hard to pin down, the kind of thing we recognize in dreams. Lightly adapted, the categories are:

- Type A. Objects smaller than us, that we can handle to see (or otherwise sense) from different angles: a kitchen utensil, a pen, an earring, a small animal, our own hands. Things we can envelop with our awareness.

- Type B. Objects usually larger than us, that we cannot freely move, but that we can move _ourselves_ around to look in on from more than one angle: the outside of a house, an elephant, a tree, a pond, a cement mixer, a mountain. Things we can lightly hold with our awareness.

- Type C. Objects larger than us, that we can move within, and construct mental maps of from many perspectives: the inside of a museum, a field, a small valley. Things that lightly hold our awareness.

- Type D. Objects larger than us, that we may move within or know in other ways, but do not sense in a unitary way (although we may perceive parts as A, B, or C objects): a forest, a town, a sea, the inside of our home, the shoreline of a river. The notes on Zubin’s talk say _there is no perspective_. These are things that envelop our awareness.

This is a typology not of physical objects but of how we think about them, so things in the world move between these categories as easily as we imagine them doing so.

James Lick was an idiosyncratic piano-maker who bought large properties around San Francisco just before the gold rush made them extremely valuable. One of his plans to memorialize himself was a pharaoh-size marble pyramid at 4th and Market, possibly flanked by colossal statues of his parents. The great geodesist George Davidson, according to legend, persuaded Lick that such monuments might be bombarded in a future war, and that a safer legacy would be an observatory. Unlike the pyramid, it’s not much to look at: just white dots on the horizon hills, doing the looking themselves.

Or that’s what they were to me. This summer, my partner and an old friend and I went to see them. You drive east from San Jose on avenues, then roads through oak savanna and occasional farms, then switchbacks. The dots turn into eggshell domes. There is parking and a little gift shop and the usual signs that grad students are around. In one dome, like a blimp in its varnished wooden hanger, is a machine that contains the largest precision lens still in general-purpose use.

It is the first artifact, the rubricated incipit, of Big Science, the kind of collaborative, complexly managed, capital-intensive thing that the Manhattan Project, CERN, and the Human Genome Project all are. You can’t touch it – you could; you don’t – but you can take a lap around the dome and see it the way you see a large tree. It becomes a Type B object within a Type C object. In the exhibits in the nearby hallways are old eyepieces and photographic plates that are even Type A objects. They make it easy to imagine a life of daily interaction with this giant device.

Now when I see the white dots on the horizon I remember walking around inside what look like dimensionless points. There is something pleasantly but undeniably uncanny about this, as there is about a realistic dollhouse. And as there is about the photographs the telescope takes. And even about a postcard of the Grand Canyon on the fridge, which still carries a detectable trace of the bewildering feeling of looking out from that rock lip.

The kind of high-resolution satellite images in this project are type A, B, C, and D objects from moment to moment: enumerable collections of manipulable, knowable pixels but also immersive representations of places so large and complex that they are functionally infinite – that contain detail and meaning beyond systematic interpretation. This disrespect for categories of scale is not unique to satellite images. But they are a very efficient way to get size-of-things decompression sickness. I think one of the reasons they feel unsettling is that they carry so much of this scale shock.

## Prospects

The geographer Jay Appleton proposed that we esthetically prefer positions that balance prospect – the opportunity to see – against refuge – the opportunity to not be seen. A pleasant place for humans in a landscape, in this concept, might be in a thick grove of trees with good sight-lines over a surrounding grassland. If this is true then perhaps it explains some of the appeal of satellite images to say that they have even wider prospects (Type D objects packed into Type A objects) and the ultimate refuge (never to have been present at the point of seeing).

Much doubt about of the satellite way of seeing starts from the problem of its disembodied viewpoint. For me, and for many others I’ve talked with about these things, the foundational critique of the view from nowhere is Donna Haraway’s massively influential _Situated Knowledges_, which comes very close to taking it as obvious that to see in this way is to see in a wrong way. One way through is to make the satellites real again: to make them subjective objects, to consider their causes and contexts, to pick apart their methods and name their perspectives. I am trying to make a small contribution in this direction here. To topicalize the WV-2/3 focal plane assembly might help, I hope, to make it a characterful thing and neither a transparent conduit of true information nor an enemy of humane sight.

People have occasionally asked me if satellite image mosaics are maps or photos. I don’t think there’s a useful answer to this. A map I guess is some kind of diagram, a cutaway of life in general, a mixture of a drawing and a story, an interpretation between dialects of space. Image mosaics might be that. Photographs are, what, faithful records of visible light at a moment? But only under big assumptions. Film photos are abstruse chemical reactions many steps removed from light, often [manipulated](https://www.youtube.com/watch?v=IoCtni-WWVs#t=2m30s), and their relationship to time is [far from simple](https://www.connolly-cleary.com/HALLofMIRRORS/LOOK.html). Photos from phones these days are getting to be almost equally complicated in their own ways. Image mosaics are – diagrams made from photos? I don’t know. I don’t think we can learn much from insisting on their categorical relationship to the idea of a map or to that of a snapshot.

I default my maps to imagery. I often think of an argument that [Robert Simmon](https://robertsimmon.com/) has made across essays and talks: that the best way to show something is usually to show it. Complex visualization techniques (diagrammatic views, false color, and so on) are particular solutions to special problems. If the task is simply to see something clearly, the starting point should be: What does it look like? Our brains and our visual systems have more than a million generations of co-adaptation. We’re extremely good at picking out the general patterns and the key details in things seen in the “normal” way, where red means red, green means green, and blue means blue. And so a preference for true color has served me well. If I want to see Sagrada Família without actually going to Barcelona, I’m looking for a photo before I’m looking for a blueprint. I use street maps when I need to navigate on streets but I don’t want to live the main of my life, to know my world, with every shape polygonized and every color painted over. I want to see the things.

One of the classic dialectics in geography is between a feature view and a field view. In the world of features, what can be known are points and crisp-edged shapes. You’re at or not; you’re inside or outside. In the world of fields, everything is continuous: attributes can be measured at points, but they never actually go from A to B without passing through something intermediate. The feature interpretation has mostly won. It certainly has some merit. And discretized geospatial objects are a snug fit with the kind of database architectures and legal frameworks that have also won.

“Raster is faster but vector is correcter” is a slogan of the feature view. They mean that fields sampled in grids – images – can be used as cached representations of reality, which they hold is more properly, more primarily, represented as objects. To me this is backward. Existence precedes essence, and the clean-ness of those clean lines is only convenienter. What we get from the world is samples. Polygons are merely masks that we put on the world to name it. This is of course a view affected by objects in my mind: years working from the point of view of satellites, which see rasters – sampled fields – that only become _things_ by our own interpretive work.

## The conversation

For a decade or so, you’ve been able to do numbers on LinkedIn by posting that satellite data in itself is irrelevant to most customers: that what they really want are not the pixels but the actionable insights derived from those pixels. This annoys me. It’s the [no take, only throw](https://cupcakelogic.tumblr.com/post/124392369931/she-is-still-learning) of data strategies. It’s true that people want answers, but it’s obvious, and it does not follow that the details of their questions are unimportant. As hard as we may try to encapsulate the work of applying particular methods on particular datasets, the process of understanding the world is irreducibly connected to the details of the world. Reality is very specific.

This is where the idea of imagery as a commodity runs into trouble. A commodity is fungible. As a buyer of grade 1 red lentils or grade C10100 electronic copper, I expect the next ton I order to be materially the same as the last one; if I learn it isn’t, I won’t buy it. As a buyer of satellite imagery, I expect the next image I order to be materially _different_ from the last one; if I learn it isn’t, I won’t buy it. Satellite imagery is much more like news than it is like beans. It’s useful insofar as it shows big slabs of reality, and those don’t funge. As I once had the pleasure of hearing the semi-legendary imagery product manager Camilla Mahon explain to an exec with more MBAs than sense, people care about what’s in the pictures they buy.

I spent most of a decade on a team making and maintaining an imagery basemap. That is a curious kind of a thing. In a basemap, you want to make something beautiful enough to be the foreground but calm enough to be the background – something self-consistent but that takes advantage of every useful data source – something specific and recent but also generalized and timeless. There’s a freedom in tasks where perfection is clearly unreachable. I got a lot of practice looking at images with these mixed needs in mind. Satellite imagery got sharper during my time at that job, but it didn’t get fundamentally better-looking, and that bothered me. Even the best data had a certain freezer-burned look that was easy to spot and hard to fix. It was the pansharpening, plus some other bits of closely related processing. It was one of three or four key upstream problems I couldn’t realistically correct as a buyer and reprocessor of imagery. We had to accept it and work on the fixable problems. It chafed a little bit for a long time.

And so one of the first projects I took on after leaving that job was a personal one: a friend had asked about certain quirks in the Pléiades Neo sample images, so I wrote down more or less every useful thing I knew about pansharpening, e-mailed it to the friend, and [posted it](https://gist.github.com/celoyd/5bb5417b24801e0446ad5977cc3581e4). My already tired joke is that it’s the worst technical introduction to pansharpening on the internet, and, because it’s the only one, also the best. That’s an exaggeration, but less than I wish it were. Pansharpening as a problem gets very little attention. Mostly it’s introduced with a conventional paragraph before we move on to the things the author actually cares about: usually a proposed solution. (As far as I know, the only paper squarely about pansharpening theory is Wald et al.’s 1997 [_Fusion of Satellite Images of Different Spatial Resolutions: Assessing the Quality of Resulting Images_](https://www.asprs.org/wp-content/uploads/pers/1997journal/jun/1997_jun_691-699.pdf). While very good, it cannot be the last word.)

What I wanted to do with that post, which it did not do, and which I still want done, is to improve pansharpening. There is an old legend about a developer who goes through a tech company’s interview process, gets hired onto a product team, fixes one particular bug on their first day, and quits before lunch. It was the only option after getting no response to a bug report they’d submitted as a user. That’s roughly my attitude with this project. People keep using fast but ugly pansharpening, so here are some easy ways to make it less ugly. If this works, soon I’ll see more beautiful images of my homeworld when I use [OSM](https://www.openstreetmap.org/)’s traceable imagery layers, Apple Maps, Google Earth, and so on. If someone wants to hire me to implement it, good: this repo is after all only noncommercially licensed, and I enjoy and am fairly good at this kind of work. But the broader goal is that I can stop seeing these bleary, crusty, groggy-looking images everywhere. I think we are close to a little renaissance in this corner of the image-processing-for-remote-sensing world, and I am trying to do my part to bring it about.

## On Earth

The main part of this project states some technical theses about pansharpening: that interpolating spectral power distributions is the right way to derive color, for example. In this more subjective corner, I make a more subjective claim: pansharpening should make satellite images a little bit more like ordinary photographs.

In my notebooks are tables, brainstormed while cooking and on BART rides, where I’ve tried to articulate something that I didn’t have a name for until I read [Fleischmann and Arribas-Bel](https://www.sciencedirect.com/science/article/pii/S0198971524000760)’s aspatial/spatial imagery distinction:

| Ordinary snapshots | Geospatial images |
| ------------------ | ----------------- |
| Bayer demosaicking | Pansharpening |
| Views from inside (buildings, landscapes, and the planetary boundary layer) | View from outside (those things) |
| Collected reactively, to record vision | Collected proactively, to extend vision |
| Not feasibly collected into self-consistent clusters | |
| Distance from sensor and thus per-object projection varies wildly across frame | Nearly orthographic perspective; everything is in the far field and functionally more or less equidistant |
| POV has an orientation to be derived from space/landscape | Space/landscape has a layout to be derived from the POV |
| Mostly 3D space with strong 2D influences (up/down is more limited than east/west or north/south) – in the tidepool | Mostly planar space, though crucially wrinkled and troubled (U/D forcibly flattened; E/W and N/S rarely obvious) – looking into the tidepool |
| Things meant to be seen: deliberate communication, even if indirectly (signs, faces, conventionally structured and painted things) | The back of the tapestry: the non-signaling consequences of life (biomes, zoning, highway routes) |
| Mostly uncalibrated images with unknown lighting  | Mostly calibrated images lit by sunlight through atmosphere |

